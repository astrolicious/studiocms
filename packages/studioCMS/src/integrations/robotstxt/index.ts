import type { AstroConfig, AstroIntegration } from 'astro';
import type { UsertAgentType } from './consts';

import fs from "node:fs";
import { fileURLToPath } from "node:url";

import { generateContent, printInfo } from './core';
import { measureExecutionTime } from './utils';

function getFileSizeInKilobytes(filename: URL): number {
  const stats = fs.statSync(filename);
  const fileSizeInBytes = stats.size;
  const fileSizeInKilobytes = fileSizeInBytes / 1024;
  return fileSizeInKilobytes;
}

export interface RobotsConfig {
  /**
   * @default false
   * @description
   * [ Optional ] Some crawlers(Yandex) support and only accept domain names.
   * @example
   * ```ts
   * integrations:[
   *  robots({
   *    host: siteUrl.replace(/^https?:\/\/|:\d+/g, "")
   *  })
   * ]
   * ```
   */
  host?: boolean | string;
  /**
   * @description
   * [ Optional, zero or more per file ] The location of a sitemap for this website.
   * @example
   * ```ts
   * sitemap: [
   *  "https://example.com/sitemap.xml",
   *  "https://www.example.com/sitemap.xml"
   * ]
   * ```
   * The value of the [SITEMAP](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#sitemap) field is case-sensitive.
   */
  sitemap?: boolean | string | string[];
  /**
   * @description
   * [ Optional ] List of `policy` rules.
   * @default
   * ```ts
   * policy:[
   *  {
   *    userAgent: "*",
   *    allow: "/"
   *  }
   * ]
   * ```
   * For more help, refer to [SYNTAX](https://yandex.com/support/webmaster/controlling-robot/robots-txt.html#recommend) by Yandex.
   */
  policy?: PolicyOptions[] | undefined;
}
export interface PolicyOptions {
  /**
   * @description
   * [ Required ] Indicates the robot to which the rules listed in `robots.txt` apply.
   * @example
   * ```ts
   * policy:[
   *  {
   *    userAgent: [
   *      'Googlebot',
   *      'Applebot',
   *      'Baiduspider',
   *      'bingbot'
   *    ],
   *    // crawling rule(s) for above bots
   *  }
   * ]
   * ```
   * Verified bots, refer to [DITIG](https://www.ditig.com/robots-txt-template#regular-template) or [Cloudflare Radar](https://radar.cloudflare.com/traffic/verified-bots).
   */
  userAgent?: UsertAgentType | (UsertAgentType)[];
  /**
   * @description
   * [ At least one or more `allow` or `disallow` entries per rule ] Allows indexing site sections or individual pages.
   * @example
   * ```ts
   * policy:[{allow:["/"]}]
   * ```
   * Path-based URL matching, refer to [SYNTAX](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#url-matching-based-on-path-values) via Google.
   */
  allow?: string | string[];
  /**
   * @description
   * [ At least one or more `disallow` or `allow` entries per rule ] Prohibits indexing site sections or individual pages.
   * @example
   * ```ts
   * policy:[
   *  {
   *    disallow:[
   *      "/admin",
   *      "/uploads/1989-08-21/*.jpg$"
   *    ]
   *  }
   * ]
   * ```
   * Path-based URL matching, refer to [SYNTAX](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#url-matching-based-on-path-values) via Google.
   */
  disallow?: string | string[];
  /**
   * @description
   * [ Optional ] Specifies the minimum interval (in seconds) for the search robot to wait after loading one page, before starting to load another.
   *
   * @example
   * ```ts
   * policy:[{crawlDelay:5}]
   * ```
   * About the [Crawl-delay](https://yandex.com/support/webmaster/robot-workings/crawl-delay.html#crawl-delay) directive.
   */
  crawlDelay?: number;
  /**
   * @description
   * [ Optional ] Indicates to the robot that the page URL contains parameters (like UTM tags) that should be ignored when indexing it.
   *
   * @example
   * ```hash
   * # for URLs like:
   * www.example2.com/index.php?page=1&sid=2564126ebdec301c607e5df
   * www.example2.com/index.php?page=1&sid=974017dcd170d6c4a5d76ae
   * ```
   * ```ts
   * policy:[
   *  {
   *    cleanParam: [
   *      "sid /index.php",
   *    ]
   *  }
   * ]
   * ```
   * For additional examples, please consult
   * Yandex's [SYNTAX](https://yandex.com/support/webmaster/robot-workings/clean-param.html#clean-param__additional) guide.
   */
  cleanParam?: string | string[];
}

const defaultConfig: RobotsConfig = {
  sitemap: true,
  host: false,
  policy: [
    {
      userAgent: "*",
      allow: "/"
    }
  ]
}

export default function createRobotsIntegration(astroConfig: RobotsConfig): AstroIntegration {

  let config: AstroConfig;
  let finalSiteMapHref: string;
  let executionTime: number;

  const megeredConfig = { ...defaultConfig, ...astroConfig };

  return {
    name: 'astrolicious/studioCMS:robotstxt',
    hooks: {
      'astro:config:setup': ({ config: cfg }) => {
        config = cfg;
      },
      'astro:build:start': () => {
        finalSiteMapHref = new URL(config.base, config.site).href;
      },
      'astro:build:done': async ({ dir, logger }) => {
        executionTime = measureExecutionTime(() => {
          fs.writeFileSync(new URL('robots.txt', dir), generateContent(megeredConfig, finalSiteMapHref, logger), 'utf-8');
        });
        const fileSize = getFileSizeInKilobytes(new URL('robots.txt', dir));
        const destDir = fileURLToPath(dir);
        printInfo(fileSize, executionTime, logger, destDir);
      },
    },
  };
}
